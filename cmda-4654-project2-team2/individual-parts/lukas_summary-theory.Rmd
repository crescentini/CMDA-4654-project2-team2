---
title: "Boosted Trees"
subtitle: "CMDA 4654 - Project 2, Group 2"
author: "Authors: Lukas Coffey, Ben Zevin, Chandler Crescentini, Juhong Hyun"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE,
        show.signif.stars = FALSE)
```

# Outline

- Summary of boosted trees
- What is boosting?
- Boosted regression theory
- Boosted classification theory
- Description of method parameters
- Using `gbm` on a small dataset
- Using gradient boosting on a large dataset

---

# Summary

Commonly referred too as gradient boosting, boosted trees is a method that works against the common problem among trees - overfitting a dataset. It's called "gradient" boosting because it uses gradient descent to iteratively find the best fit to the data. We are trying to "step" in the right direction, towards the least amount of error. The idea stems from a method called Adaboost - we will not discuss it here but you can find plenty of resources explaining the method online. 

There are two use cases for gradient tree boosting, **regression** and **classification**. The concept is the same but the implementation varies slightly depending on the use case. In this presentation we will touch on classification but focus on regression.

---

# Summary

Gradient boost for regression works by creating small regression trees (known as weak learners), calculating the errors from that weak learner, and fitting another tree to predict those errors. These models are then combined to form the gradient boosted model.

Each step of the algorithm creates a new tree that may be small or large using random samples of columns, depending on your tuning parameters. All of the trees are then scaled by a tuning parameter between 0 and 1. *This learning rate parameter basically determines how quickly our boosted trees find a good fit - the smaller the learning rate the better the model.*

The iterations continue for either **A)** a specified number of steps, or **B)** until the errors become sufficiently small.

You can see how this is similar to random forests in that the algorithm creates many trees, and can use random samples of columns. However, the algorithm does not just pick variables at random, it uses the variables that you provide. We can also visualize the importance of the variables we give the algorithm and exclude/include certain columns that provide the most prediction power. We will demonstrate this later.

---

# Pros & Cons

**Pros**

- Easily interpretable
- Robust to outliers
- Can deal with continuous and categorical
- Capture non-linear relationships well
- Scale to large datasets

**Cons**

- Can tend to overfit with predictors that have many categories
- Lack smoothness
- Can tend to have limited predictive performance
- Can take a long time to run depending on hyperparameters

---

# Terminology

There is quite a bit of mixed jargon with regards to boosted trees. Since we're using gradient descent to "boost" normal decision trees, the method is called "tree boosting." They're all referring to this same idea that we're trying to make regular decision trees better. Below are some of the terms you might see the method referred as:

- Tree boosting
- Boosted trees
- Boosted regression 
- Boosted classification
- Gradient boosting
- Gradient boosted regression trees

---
layout: false
class: inverse, middle, center

# Boosting & Gradient Descent

---

# What is Boosting?

**Key question:** Can a set of weak learners create a single strong learner?

In machine learning, boosting is an ensemble<sub>1</sub> meta-algorithm<sub>2</sub> for primarily reducing bias and variance in supervised learning, and a family of machine learning algorithms that convert weak learners<sub>3</sub> to strong learners<sub>4</sub>.

1. *Meta-algorithm:* higher-level procedure designed to find a heuristic that may provide a sufficiently good solution to an optimization problem especially with incomplete information.

2. *Ensemble:* multi-learning algorithms

3. *Weak learner:* classifier that is only slightly correlated with the true classification

4. *Strong learner:* classifier that is arbitrarily well-correlated with the true classification.


---

# What is Boosting, cont.

**Characteristics of Boosting**
- Uses lean weak classifiers with respect to a distribution
- Add them to a final strong classifier
- After a weak learner is added, data weights are readjsuted, known as "re-weighting".
- Misclassified input data gain a higher weight and examples that are classified correctly lose weight.
- Thus, future weak learners focus more on the examples that previous weak learners misclassified.

**Types of Boosting**

- AdaBoost (an adaptive boosting algorithm that won the prestigious GÃ¶del Prize. / historically significant)

- Gradient Boosting (Gradient Descent + Boosting)

- LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, etc.

---

# Key Concepts

**What is gradient descent?**

Suppose you want to optimize a function $f(x)$, assuming $f$ is differentiable, gradient descent works by iteratively find
$$x_{t+1} = x_t - \eta {\partial{f} \over \partial x}|_{x=x_t}$$
where $\eta$ is called the step size.

**What is additive model?**

Additive model: $g(x)=f_0(x)+f_1(x)+f_2(x)+...$, where the final classifier $g$ is the sum of simple base classifiers $f_i$. 

**For the boosted trees model, each base classifier is a simple decision tree.**

---

# Gradient Boosting

**Gradient Descent + Boosting = Gradient Boosting**

Similarly, if we let $g_t(x)=\sum_{i=1}^{t}{f_i(x)}$ (additive model; ensemble) be the classifier trained at iteration t, and $L(y_i,g(x_i))$ be the empirical loss function, at each iteration we will move $g_t$ towards the negative gradient direction $-{\partial L \over \partial g}|_{g=g_t}$ by $\eta$ amount.

Hence, $f_t$ is chosen to be
$$f_t = \underset{t}{argmin} \sum_{i=1}^N{[{{\partial L(y_i,g(x_i)) \over \partial g(x_i)}|_{g=g_t} }-f(x_i)]^2}$$
and the algorithm sets $g_{t+1}=g_t + \eta f_t$.

---
class: inverse, middle, center

# Boosted Regression

---

# Boosted Regression
## References
- Gradient Boosting Explained: http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/
- Gradient Boosting: https://en.wikipedia.org/wiki/Gradient_boosting
- StatQuest - Gradient Boost, Parts 1 & 2: https://www.youtube.com/watch?v=3CC4N4z3GJc&t=582s

---

# Boosted Regression - Main Idea

The main idea behind boosted regression is that we fit a model, then fit another model to the residuals, and then the combination of these becomes the new model. We iterate this idea many times to find the optimal model. In pseudocode:

**Step 1:** We fit the original model.

$$F_1(x) = y$$

**Step 2:** We fit a new model to the residuals of the model in Step 1.

$$h_1(x) = y - F_1(x)$$

**Step 3:** We combine the two previous models to find the next step in the algorithm.

$$F_2(x) = F_1(x) + h_1(x)$$

---

# Boosted Regression - Step 1

To start the regression algorithm, we initialize the model with the average of our $y$ values. This becomes our starting point because initially we want to minimize the squared error. 

$$F_0(x) = \underset{\gamma}{argmin} \sum_{i=1}^n{L(y_i, \gamma)} = \underset{\gamma}{argmin} \sum_{i=1}^n{(\gamma - y_i)^2} = \frac{1}{n}\sum_{i=1}^n{y_i}$$

Now, we can define the following models just like we did before in pseudocode.

$$F_{m+1}(x) = F_m(x) + h_m(x)=y,\ for\ m\ge0$$

where $h_m$ is a base learner (e.g. weak learner; tree).

**Note:** How do we select $M$? This value determines how many times we iterate until we find the best model. This is best done by cross-validation. We'll discuss this later in the regularization section.

---

# Boosted Regression - Step 2

After we initialize the model, $F_0(x)$, we can begin iterating. Each iteration contains 4 steps:

**Step 1:** Compute the pseudo-residuals (this is the gradient descent part of *gradient* boost).
$$r_{im} = - \left[\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}\right]_{F(x)=F_{m-1}(x)}\ for\ i=1,...,n$$

**Step 2:** Fit a base learner (weak learner, e.g. a tree) to the pseudo-residuals. This is $h_m(x)$.

**Step 3:** Compute the step magnitude multiplier, $\gamma_m$, by solving the 1D optimization problem:
$$\gamma_m = \underset{\gamma}{argmin} \sum_{i=1}^n{L(y_i,\ F_{m-1}(x_i) + \gamma h_m(x_i))}$$

**Step 4:** Update the model with the new pieces:
$$F_m(x) = F_{m-1}(x)+\gamma_mh_m(x)$$

---

# Boosted Regression - Step 3

After we compute all the iterations of the algorithm, our final model is then:
$$for\ m=1\ to\ M,\\ F_M(x)$$

with inputs:
1. Training data set, ${(x_i, y_i)}_{i=1}^{n}$
2. Loss function, $L(y, F(x))$
3. Number of iterations, $M$

We will talk about $M$ and a few more paramaters of the algorithm later.

---
class: inverse, center, middle

# Boosted Classification

---

# Boosted Classification
## References
- Boosting: https://en.wikipedia.org/wiki/Boosting_(machine_learning)#cite_note-9
- Michael Kearns(1988); Thoughts on Hypothesis Boosting, Unpublished manuscript (Machine Learning class project, December 1988)
- Gradient Boosting: https://turi.com/learn/userguide/supervised-learning/boosted_trees_regression.html


---

# Classification vs. Regression

## What is different between classification and regression?

With classification, we are not dealing with a continuous response variable. Thus we have to work with proportions and probabilities in our algorithm instead of continuous numbers. 

In the classification case, our Loss Function is:
$$L(y_i, \gamma) = -y_i\gamma\ +\ log(1+e^{\gamma})$$

where $y_i$ is a classification (0:1, yes:no, etc.), and $\gamma$ is $log(odds)$. This is the function we are trying to minimize and that we find the gradient of in our algorithm.

This changes our initial value when we initialize the algorithm to a log odds value:
$$F_0(x) = log(odds)$$

where $odds$ is the ratio of the sums of the categories (# of yes's/# of no's, # of 1's/# of 0's, etc.).

---

# Classification vs. Regression cont.

The steps for the algorithm are the same - they just use the new Loss function instead. We calculate the residuals for the previous step, and the next step uses the variables from the data to predict those residuals.

---
class: inverse, center, middle

# Regularization

---

# Regularization

One of the main goals in using boosted trees is to avoid overfitting the data. To achieve a boosted model that is well-generalized and not overfitted, we can make use of **regularization**. These are just a few of the regularization parameters that we can use to control effectiveness of the boosting algorithm:
- The number of iterations, $M$
- Maximum tree depth
- Minimum number of observations in tree leaves
- Penalizing complexity: pruning back the weak learners that don't provide any benefit
- Shrinkage parameter, $\nu$, known as the *"learning rate"*
- Size of the trees

**The number of iterations, $M$**

This is the number of times we should run the gradient descent algorithm (basically the number of trees we create). As stated previously, $M$ is most commonly chosen using cross-validation. Increasing $M$ reduces error on the training set, but making $M$ too high can lead to overfitting.

---

# Regularization Parameters

**Maximum tree depth**

This is the maximum distance from the root node to the furthest leaf node in each of our trees. This helps control how "weak" our weak learners should actually be.

**Minimum number of observations in tree leaves**

This parameter controls the minimum number of observations within each leaf of the weak learners. This tells the weak learners to basically ignore any splits or new terminal nodes that have fewer observations than this number. This helps to reduce variance in the predictions at the leaves.

**Penalizing complexity: pruning back the weak learners that don't provide any benefit**

This is defined as the "proportional number of leaves in the learned trees." We can employ a post-pruning algorithm that removes branches failing to reduce the loss by a certain threshold. This basically helps to keep the trees from becoming too strong and overfitting.

---

# Regularization Parameters, cont.

**Shrinkage parameter, $\nu$, known as the "learning rate"**

This is arguably one of the most important parameters. This parameter is a value between 0 and 1, and controls how fast or slow our boosted algorithm finds the optimal model. A lower learning rate (e.g. 0.1) means higher computation time and more iterations, but a better and more generalized model. It is applied to the algorithm as:
$$F_m(x) = F_{m-1}(x)+\nu\cdot\gamma_mh_m(x)$$

**Size of the trees**

The tree size parameter, $J$, controls the number of terminal nodes in each weak learner. $J=2$ would result in each tree being a stump, and no variable-variable interaction. $J=3$ would allow for a maximum of 2 variables per tree, and a max of 3 leaves. This is a strong controller of the complexity of the trees in the algorithm and coupled with the learning rate can have a big effect on computation time.

---
class: inverse, center, middle

# Small Example - Boston Housing Data

---

# Boston Housing Data

```{r, echo = FALSE, message = FALSE}
library(gbm)
library(MASS)
library(randomForest)
library(knitr)
```

We will use the `gbm` function to run the gradient boosting algorithm on the Boston Housing dataset.

```{r, highlight.output = 6}
set.seed(101)

train <- sample(1:506, size = 354) # 70% train, 30% test

boston_fit <- gbm(medv ~ ., data = Boston[train,], distribution = "gaussian",
                  n.trees = 10000, shrinkage = 0.01, interaction.depth = 2,
                  bag.fraction = 1.0, train.fraction = 1.0, n.minobsinnode = 5,
                  cv.folds = 5)
boston_fit
```

---

# Variable Importance

.pull-left[
```{r}
fit_sum <- summary(boston_fit)
```
]

.pull-right[
```{r, highlight.output = c(2,3)}
data.frame(var = fit_sum$var,
           rel.inf = fit_sum$rel.inf)
```
]

---

# Partial Dependence Plots

.pull-left[
```{r, eval = FALSE}
plot(boston_fit, i = "lstat", main = "Partial Dependence Plot for lstat")
```
```{r, echo = FALSE}
plot(boston_fit, i = "lstat", main = "Partial Dependence Plot for lstat")
```
]

.pull-right[
```{r, eval = FALSE}
plot(boston_fit, i = "rm", main = "Partial Dependence Plot for rm")
```
```{r, echo = FALSE}
plot(boston_fit, i = "rm", main = "Partial Dependence Plot for rm")
```
]

---
class: center, middle

# Optimal Number of Boosting Iterations

```{r, eval = FALSE}
best.iter <- gbm.perf(boston_fit, method = "cv")
```
```{r, echo = FALSE}
best.iter <- gbm.perf(boston_fit, method = "cv")
title("GBM Performance")
legend("topright", legend = c("CV Error", "Validation Error", "Optimal Iterations"), col = c("green", "black", "blue"), lty = c(1,1,2))
```
---

# Optimal Number, cont.

&nbsp;

&nbsp;

&nbsp;

```{r, highlight.output = TRUE}
pred <- predict.gbm(boston_fit, Boston[-train,], n.trees = best.iter)

error <- sum((pred - Boston$medv[-train])^2)
print(error)
```

---
# Parameter Fine Tuning

```{r}
boston_fit1 <- gbm(medv ~ ., data = Boston[train,], distribution = "gaussian",
                   n.trees = 10000, shrinkage = 0.01, interaction.depth = 2,
                   bag.fraction = 1.0, train.fraction = 1.0, n.minobsinnode = 5,
                   cv.folds = 5)

boston_fit2 <- gbm(medv ~ ., data = Boston[train,], distribution = "gaussian",
                   n.trees = 5000, shrinkage = 0.01, interaction.depth = 2,
                   bag.fraction = 1.0, train.fraction = 1.0, n.minobsinnode = 5,
                   cv.folds = 5)

boston_fit3 <- gbm(medv ~ ., data = Boston[train,], distribution = "gaussian",
                   n.trees = 10000, shrinkage = 0.01, interaction.depth = 2,
                   bag.fraction = 1.0, train.fraction = 1.0, n.minobsinnode = 10,
                   cv.folds = 5)

boston_fit4 <- gbm(medv ~ ., data = Boston[train,], distribution = "gaussian",
                   n.trees = 5000, shrinkage = 0.1, interaction.depth = 2,
                   bag.fraction = 1.0, train.fraction = 1.0, n.minobsinnode = 10,
                   cv.folds = 5)

```

---
class: center

# Parameter Fine Tuning, cont.

&nbsp;

&nbsp;

&nbsp;

```{r, include = FALSE}
best.iter2 <- gbm.perf(boston_fit2, method = "cv")
best.iter3 <- gbm.perf(boston_fit3, method = "cv")
best.iter4 <- gbm.perf(boston_fit4, method = "cv")
```

```{r, echo = FALSE}
pred2 <- predict.gbm(boston_fit2, Boston[-train,], n.trees = best.iter2, type = "response")
pred3 <- predict.gbm(boston_fit3, Boston[-train,], n.trees = best.iter3, type = "response")
pred4 <- predict.gbm(boston_fit4, Boston[-train,], n.trees = best.iter4, type = "response")

error2 <- sum((pred2 - Boston$medv[-train])^2)
error3 <- sum((pred3 - Boston$medv[-train])^2)
error4 <- sum((pred4 - Boston$medv[-train])^2)
```

```{r, echo = FALSE, highlight.output = 3}
df <- data.frame(model = c("boston_fit1", "boston_fit2", "boston_fit3", "boston_fit4"), n.trees = c(10000, 5000, 10000, 5000), shrinkage = c(0.01, 0.01, 0.01, 0.1), n.minobsinnode = c(5, 5, 10, 10), best_iter = c(best.iter, best.iter2, best.iter3, best.iter4), error = c(error, error2, error3, error4))

knitr::kable(df, format = "html")
```


---

# Comparison with Random Forest

```{r}
rf <- randomForest(medv ~ ., data = Boston[train,], distribution = "gaussian", 
                   n.trees = 5000, shrinkage = 0.01, interaction.depth = 2,
                   nodesize = 5)
```

.pull-left[
```{r, highlight.output = c(7,14), echo = FALSE}
rf$importance
```
]

.pull-right[
```{r, echo = FALSE}
plot(rf$importance, ylab = "IncNodePurity", main = "Variable Importance Plot", cex = 2, pch = 20)
```
]

---

# Comparison with Random Forest, cont.

&nbsp;

&nbsp;

&nbsp;

```{r, highlight.output = TRUE}
predRF <- predict(rf, Boston[-train,], type = "response")

errorRF <- sum((predRF - Boston$medv[-train])^2)
print(errorRF)
```






