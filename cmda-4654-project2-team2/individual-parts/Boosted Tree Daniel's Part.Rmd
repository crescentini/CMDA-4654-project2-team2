---
title: "Presentation Slides"
subtitle: "Boosted Classification (Daniel)"
author: "Juhong Hyun"
institute: "CMDA 4654"
date: "2019/11/30"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

# Boosted Classification
## References
- Boosting: https://en.wikipedia.org/wiki/Boosting_(machine_learning)#cite_note-9
- Michael Kearns(1988); Thoughts on Hypothesis Boosting, Unpublished manuscript (Machine Learning class project, December 1988)
- Gradient Boosting: https://turi.com/learn/userguide/supervised-learning/boosted_trees_regression.html


---

# What is boosting?

Key question:

--

 Can a set of weak learners create a single strong learner?

--

In machine learning, boosting is an **ensemble**<sub>1</sub> meta-algorithm<sub>2</sub> for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners<sub>3</sub> to **strong learners**<sub>4</sub>.

--

.footnote[
[1] meta-algorithm: higher-level procedure designed to find a heuristic that may provide a sufficiently good solution to an optimization problem especially with incomplete information.

[2] Ensemble: multi-learning algorithms

[3] Weak learner: classifier that is only slightly correlated with the true classification

[4] Strong learner: classifier that is arbitrarily well-correlated with the true classification.
]

---

# What are distinct features of boosting?

--

- Lean weak classifiers with respect to a distribution

--

- Add them to a final strong classifier

--

- After a weak learner is added, data weights are readjsuted, known as "re-weighting".

--

- Misclassified input data gain a higher weight and examples that are classified correctly lose weight.

--

- Thus, future weak learners focus more on the examples that previous weak learners misclassified.

---

# What kind of boosting are there?


- The original ones(not adaptive and could not take full advantage of the weak learners)

--

1.a recursive majority gate formaulation

--

2.boost by majority

- **AdaBoost**(an adaptive boosting algorithm that won the prestigious GÃ¶del Prize. / historically significant)

- Gradient Boosting (Gradient Descent + Boosting)

- LPBoost, TotalBoost, BrownBoost, xgboost, MadaBoost, etc.

---

# Gradient Boosting

In this presentation, we will focus on to the gradient boosting.

- What is gradient descent?

Suppose you want to optimize a function $f(x)$, assuming $f$ is differentiable, gradient descent works by iteratively find
$$x_{t+1} = x_t - \eta {\partial{f} \over \partial x}|_{x=x_t}$$
where $\eta$ is called the step size.

- What is additive model?

Additive model:
$$g(x)=f_0(x)+f_1(x)+f_2(x)+...$$

where the final classifier $g$ is the sum of simple base classifiers $f_i$. For boosted trees model, each base classifier is a simple decision tree.

---

# Gradient Boosting = Gradient descent + Boosting

Similarly, if we let $g_t(x)=\sum_{i=0}^{t-1}{f_i(x)}$ (additiv model; esemble) be the classifier trained at iteration t, and $L(y_i,g(x_i))$ be the empirical loss function, at each iteration we will move $g_t$ towards the negative gradient direction $-{\partial L \over \partial g}|_{g=g_t}$ by $\eta$ amount.

Hence, $f_t$ is chosen to be
$$f_t = \underset{t}{argmin} \sum_{i=1}^N{[{{\partial L(y_i,g(x_i)) \over \partial g(x_i)}|_{g=g_t} }-f(x_i)]^2}$$
and the algorithm sets $g_{t+1}=g_t + \eta f_t$.

---

# What is different between classification and regression?

For regression problems with squared loss function, ${\partial L(y_i,g(x_i)) \over \partial g(x_i)}$ is simply $y_i - g(x_i)$. The algorithm simply fit a new decision tree to the residual at each iteration.


